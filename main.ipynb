{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN ONCE\n",
    "\n",
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"debashishsau/aslamerican-sign-language-aplhabet-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "# move the dataset to a new location\n",
    "\n",
    "import shutil\n",
    "shutil.move(path, \"debashishsau_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure you have CUDA available\n",
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(f\"PyTorch CUDA version: {torch.version.cuda}\")\n",
    "print(f\"PyTorch built with CUDA: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NOTICE \n",
    "# 1. clone https://github.com/guglielmocamporese/hands-segmentation-pytorch \n",
    "# 2. run ./scripts/download_model_checkpoint.sh from hands directory in bash. Make sure to have the conda environment activated, with gdown installed.\n",
    "# if you don't do this, you'll get a bunch of blank images.\n",
    "\n",
    "# !python hands/main.py --mode predict --data_base_path debashishsau_dataset/ASL_Alphabet_Dataset/asl_alphabet_train/A --model_checkpoint \"hands/checkpoint/checkpoint.ckpt\" --model_pretrained\n",
    "\n",
    "### NOW YOU MUST SEGMENT THE DATA USING THE HANDS MODEL AND SAVE THE SEGMENTED DATA TO A NEW LOCATION.\n",
    "# now loop this for every letter\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "for letter in tqdm(os.listdir(\"debashishsau_dataset/ASL_Alphabet_Dataset/asl_alphabet_train\")):\n",
    "    path = os.path.join(\"debashishsau_dataset/ASL_Alphabet_Dataset/asl_alphabet_train/\", letter)\n",
    "    !python hands/main.py --mode predict --data_base_path $path --model_checkpoint \"hands/checkpoint/checkpoint.ckpt\" --model_pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the regular data and the segmented data, and make overlap so that the pixels that are white in the segmented data are kept as normal in the regular data.\n",
    "# and the pixels that are black in the segmented data are set to black in the regular data.\n",
    "# note that the regular data is for ex xxx.jpg, and the segmented data is for ex xxx_pred.png, in the same location.\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def overlap_images(data_path, segmented_path, output_path):\n",
    "    for root, dirs, files in os.walk(data_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".jpg\"):\n",
    "                file_name = file.replace(\".jpg\", \"\")\n",
    "                img = cv2.imread(os.path.join(root, file))\n",
    "                seg = cv2.imread(os.path.join(segmented_path, file_name + \"-pred.png\"))\n",
    "              \n",
    "                # cv2.imshow('Result', seg)\n",
    "                # cv2.waitKey(0)\n",
    "                # cv2.destroyAllWindows()\n",
    "                mask = np.all(seg == 0, axis=-1)\n",
    "                \n",
    "                processed_img = img.copy()\n",
    "                processed_img[mask] = [0,0,0]\n",
    "                \n",
    "                # if output dir doesn't exist yet, create it\n",
    "                if not os.path.exists(output_path):\n",
    "                    os.makedirs(output_path)\n",
    "                    \n",
    "                    \n",
    "                cv2.imwrite(os.path.join(output_path, file), processed_img)\n",
    "                \n",
    "               \n",
    "               \n",
    "data_path = \"debashishsau_dataset/ASL_Alphabet_Dataset/asl_alphabet_train/\" \n",
    "for letter in os.listdir(data_path):\n",
    "    overlap_images(os.path.join(data_path, letter), os.path.join(data_path, letter), os.path.join(data_path, letter + \"_processed\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move the processed data to a new location, only move folders that end with _processed\n",
    "\n",
    "import shutil\n",
    "data_path = \"debashishsau_dataset/ASL_Alphabet_Dataset/asl_alphabet_train/\"\n",
    "for letter in os.listdir(data_path):\n",
    "    if letter.endswith(\"_processed\"):\n",
    "        shutil.move(os.path.join(data_path, letter), \"debashishsau_dataset_processed\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training and validation\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "data_path = \"debashishsau_dataset_processed\"\n",
    "train_path = \"debashishsau_dataset_processed_train\"\n",
    "val_path = \"debashishsau_dataset_processed_val\"\n",
    "\n",
    "if not os.path.exists(train_path):\n",
    "    os.makedirs(train_path)\n",
    "if not os.path.exists(val_path):\n",
    "    os.makedirs(val_path)\n",
    "    \n",
    "for letter in os.listdir(data_path):\n",
    "    if not letter.startswith(\".\"):\n",
    "        files = os.listdir(os.path.join(data_path, letter))\n",
    "        random.shuffle(files)\n",
    "        split = int(0.8 * len(files))\n",
    "        train_files = files[:split]\n",
    "        val_files = files[split:]\n",
    "        \n",
    "        for file in train_files:\n",
    "            if not os.path.exists(os.path.join(train_path, letter)):\n",
    "                os.makedirs(os.path.join(train_path, letter))\n",
    "            shutil.move(os.path.join(data_path, letter, file), os.path.join(train_path, letter, file))\n",
    "        for file in val_files:\n",
    "            if not os.path.exists(os.path.join(val_path, letter)):\n",
    "                os.makedirs(os.path.join(val_path, letter))\n",
    "            shutil.move(os.path.join(data_path, letter, file), os.path.join(val_path, letter, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete useless images that are more than 90% black. Don't forget to also delete form the val folder by changing the data_dir\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "data_dir = \"debashishsau_dataset_processed_train\"\n",
    "# data_dir = \"debashishsau_dataset_processed_val\" # then delete from val\n",
    "\n",
    "# if image is more than 90% black, useless\n",
    "\n",
    "useless = 0\n",
    "total = 0\n",
    "for letter in os.listdir(data_dir):\n",
    "    for file in os.listdir(os.path.join(data_dir, letter)):\n",
    "        # if file is a nothing instance, skip\n",
    "        if letter == \"nothing_processed\":\n",
    "            continue\n",
    "        img = cv2.imread(os.path.join(data_dir, letter, file), cv2.IMREAD_GRAYSCALE)\n",
    "        total += 1\n",
    "         # Calculate the total number of pixels\n",
    "        total_pixels = img.size\n",
    "\n",
    "        # Count the number of black pixels (intensity value of 0)\n",
    "        black_pixels = np.sum(img == 0)\n",
    "\n",
    "        # Calculate the percentage of black pixels\n",
    "        black_percentage = (black_pixels / total_pixels) * 100\n",
    "\n",
    "        if black_percentage > 90:\n",
    "            # # 1/200 chance of showing image\n",
    "            # if np.random.randint(0, 200) == 0:\n",
    "            #     cv2.imshow('Result', img)\n",
    "            #     cv2.waitKey(0)\n",
    "            #     cv2.destroyAllWindows()\n",
    "            useless += 1\n",
    "            os.remove(os.path.join(data_dir, letter, file)) # uncomment to delete\n",
    "        \n",
    "print(f\"Useless images removed: {useless}\")\n",
    "print(f\"Total images: {total}\")\n",
    "print(f\"Percentage useless: {useless / total * 100}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case you want to check the black percentage of a specific image\n",
    "\n",
    "specific_file_path= \"debashishsau_dataset_processed_val\\E_processed\\E (77).jpg\"\n",
    "\n",
    "# test black % of this image\n",
    "img = cv2.imread(specific_file_path, cv2.IMREAD_GRAYSCALE)\n",
    "total_pixels = img.size\n",
    "black_pixels = np.sum(img == 0)\n",
    "black_percentage = (black_pixels / total_pixels) * 100\n",
    "print(f\"Black percentage of {specific_file_path}: {black_percentage}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By this stage your preprocessed data should be ready for training in the folder \n",
    "# debashishsau_dataset_processed_train and debashishsau_dataset_processed_val\n",
    "\n",
    "# transfer learing from resnet18, 512 to one layer of 256 neurons, to 128, and then 29 neurons for the 29 classes\n",
    "#once your model is done, move the model and plot file to the models and plots dirs respectively so you don't accidentally overwrite them later\n",
    "\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import time\n",
    "import os\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = \"debashishsau_dataset_processed\"\n",
    "\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir + '_' + x), data_transforms[x]) for x in ['train', 'val']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            \n",
    "            if phase == 'train':\n",
    "                training_losses.append(epoch_loss)\n",
    "            else:\n",
    "                validation_losses.append(epoch_loss)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    return model\n",
    "\n",
    "model_ft = models.resnet18(pretrained=True)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "print(num_ftrs)\n",
    "\n",
    "# 512 -> 256 -> 128 -> 29\n",
    "\n",
    "model_ft.fc = nn.Sequential(\n",
    "    nn.Linear(num_ftrs, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 29),\n",
    ")\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "\n",
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                          num_epochs=22)\n",
    "\n",
    "\n",
    "\n",
    "# save the model\n",
    "torch.save(model_ft, \"asl_model_256_128.pth\")\n",
    "\n",
    "# show plot of training and validation loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#save plot to file\n",
    "plt.plot(training_losses, label=\"Training Loss\")\n",
    "plt.plot(validation_losses, label=\"Validation Loss\")\n",
    "plt.legend()\n",
    "plt.savefig(\"loss_plot.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# segment the test data\n",
    "# remember test folder is flat, no subfolders\n",
    "# Download any dataset from the internet and put it in a folder of your choosing.\n",
    "# make sure to change the name of the folder in the test_dir variable below\n",
    "# make sure the files are flat in the folder, no subfolders, and that the files are .jpg\n",
    "# also make sure the first letter of the file is the letter it represents\n",
    "\n",
    "import os\n",
    "\n",
    "test_dir = \"misc_test\"\n",
    " \n",
    "for file in os.listdir(test_dir):\n",
    "    if file.endswith(\".jpg\"):\n",
    "        !python hands/main.py --mode predict --data_base_path $test_dir --model_checkpoint \"hands/checkpoint/checkpoint.ckpt\" --model_pretrained\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do segmentation preprocessing on the test data as well\n",
    "# again don't forget to change the dir name as needed\n",
    "# overlap images in test that end in -pred.png with the original images\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def overlap_images(data_path, segmented_path, output_path):\n",
    "    # files are flat in test dir\n",
    "    \n",
    "    for file in os.listdir(data_path):\n",
    "        if file.endswith(\".jpg\"):\n",
    "            file_name = file.replace(\".jpg\", \"\")\n",
    "            img = cv2.imread(os.path.join(data_path, file))\n",
    "            seg = cv2.imread(os.path.join(segmented_path, file_name + \"-pred.png\"))\n",
    "            # cv2.imshow('Result', seg)\n",
    "            # cv2.waitKey(0)\n",
    "            # cv2.destroyAllWindows()\n",
    "            mask = np.all(seg == 0, axis=-1)\n",
    "\n",
    "            processed_img = img.copy()\n",
    "            processed_img[mask] = [0,0,0]\n",
    "\n",
    "            # if output dir doesn't exist yet, create it\n",
    "            if not os.path.exists(output_path):\n",
    "                os.makedirs(output_path)\n",
    "\n",
    "\n",
    "            cv2.imwrite(os.path.join(output_path, file), processed_img)\n",
    "            \n",
    "test_dir = \"roboflow_test\"\n",
    "# test_dir = \"misc_test\"\n",
    "overlap_images(test_dir, test_dir, \"test_processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove images that are more than 90% black from the test data\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "data_dir = \"test_processed\"\n",
    "\n",
    "useless = 0\n",
    "total = 0\n",
    "\n",
    "for file in os.listdir(data_dir):\n",
    "    img = cv2.imread(os.path.join(data_dir, file), cv2.IMREAD_GRAYSCALE)\n",
    "    total += 1\n",
    "     # Calculate the total number of pixels\n",
    "    total_pixels = img.size\n",
    "\n",
    "    # Count the number of black pixels (intensity value of 0)\n",
    "    black_pixels = np.sum(img == 0)\n",
    "\n",
    "    # Calculate the percentage of black pixels\n",
    "    black_percentage = (black_pixels / total_pixels) * 100\n",
    "\n",
    "    if black_percentage > 90:\n",
    "        # # 1/200 chance of showing image\n",
    "        # if np.random.randint(0, 200) == 0:\n",
    "        #     cv2.imshow('Result', img)\n",
    "        #     cv2.waitKey(0)\n",
    "        #     cv2.destroyAllWindows()\n",
    "        useless += 1\n",
    "        os.remove(os.path.join(data_dir, file)) # uncomment to delete\n",
    "        \n",
    "print(f\"Useless images removed: {useless}\")\n",
    "print(f\"Total images: {total}\")\n",
    "print(f\"Percentage useless: {useless / total * 100}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the images in the test_processed folder\n",
    "# uncomment the cv2.imshow lines to see the images with the prediction and actual letter\n",
    "# you can also change that if statement to test a specific letter\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "model = torch.load(\"models/asl_model_256.pth\")\n",
    "model.eval()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "data_path = \"test_processed\"\n",
    "class_names = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space']\n",
    "correct = 0\n",
    "total = 0\n",
    "for file in os.listdir(data_path):\n",
    "    img = Image.open(os.path.join(data_path, file))\n",
    "    if img.mode != 'RGB':\n",
    "        img = img.convert('RGB')\n",
    "    img = transform(img)\n",
    "    img = img.unsqueeze(0)\n",
    "    output = model(img.cuda())\n",
    "    _, pred = torch.max(output, 1)\n",
    "    \n",
    "    # get the letter from the file name\n",
    "    letter = file[0]\n",
    "    \n",
    "    if class_names[pred] == letter:\n",
    "        correct += 1\n",
    "    \n",
    "    # show image with prediction in image\n",
    "    # if (letter == 'D'):\n",
    "    #     img = cv2.imread(os.path.join(data_path, file))\n",
    "    #     cv2.putText(img, 'pred ' + class_names[pred], (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "    #     cv2.putText(img, 'actual ' + letter, (200, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "    #     cv2.imshow('Result', img)\n",
    "    #     cv2.waitKey(0)\n",
    "    #     cv2.destroyAllWindows()\n",
    "        \n",
    "    total += 1\n",
    "    \n",
    "print(f\"Accuracy: {correct / total * 100}%\")\n",
    "    \n",
    "    \n",
    "    # cv2.imshow('Result', cv2.imread(os.path.join(data_path, file)))\n",
    "    # cv2.waitKey(0)\n",
    "    # cv2.destroyAllWindows()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model\n",
    "# show webcam feed and predict the sign language letter\n",
    "# this does not work well since the segmentation model is rather slow\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "model = torch.load(\"models/asl_model_256.pth\")\n",
    "model.eval()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "class_names = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space']\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "    \n",
    "    img = Image.fromarray(frame)\n",
    "    img = transform(img)    \n",
    "    img = img.unsqueeze(0)\n",
    "    \n",
    "    # persist to webcam folder\n",
    "    if not os.path.exists(\"webcam\"):\n",
    "        os.makedirs(\"webcam\")\n",
    "    \n",
    "    cv2.imwrite(\"webcam/webcam.jpg\", frame)\n",
    "    \n",
    "    result = subprocess.run(\n",
    "        [\"python\", \"./hands/main.py\", \"--mode\", \"predict\", \"--data_base_path\", \"webcam\", \"--model_checkpoint\", \"hands/checkpoint/checkpoint.ckpt\", \"--model_pretrained\"],  # Command to run the script,\n",
    "        text=True,               # Ensures the output is captured as a string\n",
    "        capture_output=True,      # Captures stdout and stderr\n",
    "    )\n",
    "\n",
    "    segmented_img_data = result.stdout\n",
    "    data_path = \"webcam\"\n",
    "\n",
    "    # overlap segmented webcam with original webcam\n",
    "    for file in os.listdir(data_path):\n",
    "        if file.endswith(\".jpg\"):\n",
    "            file_name = file.replace(\".jpg\", \"\")\n",
    "            img = cv2.imread(os.path.join(data_path, file))\n",
    "            \n",
    "            # cv2.imshow('Result', seg)\n",
    "            # cv2.waitKey(0)\n",
    "            # cv2.destroyAllWindows()\n",
    "            mask = np.all(segmented_img_data == 0, axis=-1)\n",
    "\n",
    "            processed_img = img.copy()\n",
    "            processed_img[mask] = [0,0,0]\n",
    "    \n",
    "    processed_img = Image.fromarray(processed_img)\n",
    "    processed_img = transform(processed_img)\n",
    "    processed_img = processed_img.unsqueeze(0)\n",
    " \n",
    "    with torch.no_grad():\n",
    "        outputs = model(processed_img.cuda())\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        print(class_names[preds.item()])\n",
    "        \n",
    "        cv2.putText(frame, class_names[preds.item()], (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "        cv2.imshow('Result', frame)\n",
    "        \n",
    "        \n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edit this as needed if you inadvetently created masks\n",
    "\n",
    "delete_dir = \"blahblahblah\"\n",
    "\n",
    "import os\n",
    "for filename in os.listdir(delete_dir):\n",
    "    if 'pred' in filename:\n",
    "        os.remove(f'{delete_dir}/{filename}')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "signitup",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
